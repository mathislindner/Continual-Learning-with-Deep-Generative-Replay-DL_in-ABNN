{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement https://arxiv.org/pdf/1705.08690.pdf on avalanche framework on permuted MNIST\n",
    "# https://aahaanmaini.medium.com/mimicking-human-continual-learning-in-a-neural-network-c15e1ae11d70\n",
    "#continual learning\n",
    "from avalanche.benchmarks.classic import PermutedMNIST, SplitMNIST\n",
    "from avalanche.models import SimpleMLP\n",
    "\n",
    "\n",
    "# CL Benchmark Creation\n",
    "BM = SplitMNIST(n_experiences=5, seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create naive and deepgen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "#Naive approach\n",
    "model_naive = SimpleMLP(num_classes=10, hidden_size = 400, hidden_layers=2)\n",
    "\n",
    "#Continual Learning approach\n",
    "model_cl_scholar = SimpleMLP(num_classes=10, hidden_size = 400, hidden_layers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"data/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Print the model\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import optimizers\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 4, 4], expected input[1, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15264\\2684336931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Forward pass real batch through D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_cpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Calculate loss on all-real batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0merrD_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15264\\665811006.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Create the Discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathi\\Anaconda3\\envs\\DL2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 454\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 4, 4], expected input[1, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "dataloader = BM.original_train_dataset\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import accuracy_metrics\n",
    "\n",
    "optimizer = SGD(model_naive.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Continual learning strategy\n",
    "cl_strategy_naive = Naive(\n",
    "    model_naive, optimizer, criterion, train_mb_size=32, train_epochs=2, \n",
    "    eval_mb_size=32)\n",
    "\n",
    "# model_cl_scholar strategy\n",
    "cl_strategy_scholar = Naive(\n",
    "    model_cl_scholar, optimizer, criterion, train_mb_size=32, train_epochs=2,\n",
    "    eval_mb_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks learned so far: [0, 7]\n",
      "-- >> Start of training phase << --\n",
      "662it [01:52,  5.86it/s]                         \n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0616\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9829\n",
      "100%|██████████| 381/381 [00:08<00:00, 42.95it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0149\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9951\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 52.28it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0136\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9960\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 66/66 [00:01<00:00, 44.13it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 1.0057\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.6775\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 49.60it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.0889\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0015\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 62/62 [00:01<00:00, 49.30it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 8.4194\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 60/60 [00:01<00:00, 41.43it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 6.5008\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0037\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.3456\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.3428\n",
      "Tasks learned so far: [0, 1, 6, 7]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 396/396 [00:11<00:00, 34.29it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0513\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9862\n",
      "100%|██████████| 396/396 [00:11<00:00, 35.80it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0127\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9972\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 44.63it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9864\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.7460\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 66/66 [00:01<00:00, 41.00it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0202\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9952\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 48.29it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.2734\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0109\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 62/62 [00:01<00:00, 41.00it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.3009\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 60/60 [00:01<00:00, 43.56it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.2074\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0168\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 2.9144\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.3635\n",
      "Tasks learned so far: [0, 1, 2, 4, 6, 7]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 369/369 [00:10<00:00, 36.47it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2368\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9336\n",
      "100%|██████████| 369/369 [00:10<00:00, 36.79it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0487\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9857\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:02<00:00, 29.97it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.1370\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.2440\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 66/66 [00:01<00:00, 44.34it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.8099\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0010\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 43.93it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0418\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.9881\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 62/62 [00:01<00:00, 44.96it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.7610\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 60/60 [00:01<00:00, 41.89it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.2052\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0026\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 3.7775\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2487\n",
      "Tasks learned so far: [0, 1, 2, 4, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 369/369 [00:13<00:00, 28.33it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3183\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9160\n",
      "100%|██████████| 369/369 [00:12<00:00, 30.20it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0610\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9797\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 37.83it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.6135\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0867\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 66/66 [00:01<00:00, 33.38it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.7566\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 42.11it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 7.9094\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 62/62 [00:01<00:00, 35.04it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0447\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.9834\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 60/60 [00:01<00:00, 38.69it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.5189\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0016\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.1929\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2127\n",
      "Tasks learned so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 361/361 [00:10<00:00, 35.92it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2918\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9219\n",
      "100%|██████████| 361/361 [00:09<00:00, 37.51it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9697\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 47.02it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.3899\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1071\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 66/66 [00:01<00:00, 45.64it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.7895\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 63/63 [00:01<00:00, 41.85it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.4905\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 62/62 [00:01<00:00, 49.05it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.4066\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0570\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 60/60 [00:01<00:00, 44.92it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0565\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9753\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.0732\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2183\n"
     ]
    }
   ],
   "source": [
    "#train naive model on train stream\n",
    "#train GAN on 50% train stream data and 50% generated data\n",
    "#train CL on 50% GAN generated data and 50% train stream data\n",
    "\n",
    "results = []\n",
    "tasks_learned_so_far = []\n",
    "\n",
    "# iterating over the train stream\n",
    "for experience in BM.train_stream:\n",
    "    print('Tasks learned so far:', experience.classes_seen_so_far)    \n",
    "\n",
    "    #train naive model on train stream\n",
    "    cl_strategy_naive.train(experience, verbose=0)\n",
    "    #test naive model on test stream\n",
    "    naive_evaluation = cl_strategy_naive.eval(BM.test_stream)\n",
    "\n",
    "    #train GAN on 50% train stream data and 50% generated data\n",
    "    #train CL on 50% GAN generated data and 50% train stream data\n",
    "    if experience.task_label == 0:\n",
    "        #train gan only first task\n",
    "        gan.fit(experience, verbose=0)\n",
    "        combined_data = experience\n",
    "    else:\n",
    "        new_data = gan.generate()\n",
    "        combined_data = np.concatenate((experience, new_data), axis=0)\n",
    "    cl_strategy_scholar.train(combined_data, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9931079750574335,\n",
       "  'Loss_Epoch/train_phase/train_stream/Task000': 0.025623612313078668,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.9945219123505976,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 0.01716251609693256,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001': 0.1054,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task001/Exp001': 3.032204005432129,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002': 0.095,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task002/Exp002': 3.314279815673828,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003': 0.0869,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task003/Exp003': 3.098995807647705,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004': 0.1006,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task004/Exp004': 3.020626127624512,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.9945219123505976,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task001': 0.1054,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task002': 0.095,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task003': 0.0869,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task004': 0.1006,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task000': 0.01716251609693256,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task001': 3.032204005432129,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task002': 3.314279815673828,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task003': 3.098995807647705,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task004': 3.020626127624512},\n",
       " {'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9922590837282781,\n",
       "  'Loss_Epoch/train_phase/train_stream/Task000': 0.02579351594012472,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.062236407838495,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001': 0.1054,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task001/Exp001': 3.032204005432129,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002': 0.095,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task002/Exp002': 3.314279815673828,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003': 0.0869,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task003/Exp003': 3.098995807647705,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004': 0.1006,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task004/Exp004': 3.020626127624512,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.0,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task001': 0.1054,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task002': 0.095,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task003': 0.0869,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task004': 0.1006,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task000': 7.062236407838495,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task001': 3.032204005432129,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task002': 3.314279815673828,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task003': 3.098995807647705,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task004': 3.020626127624512},\n",
       " {'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9777966101694915,\n",
       "  'Loss_Epoch/train_phase/train_stream/Task000': 0.06993110088094816,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.242052741259693,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001': 0.1054,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task001/Exp001': 3.032204005432129,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002': 0.095,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task002/Exp002': 3.314279815673828,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003': 0.0869,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task003/Exp003': 3.098995807647705,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004': 0.1006,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task004/Exp004': 3.020626127624512,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.0,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task001': 0.1054,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task002': 0.095,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task003': 0.0869,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task004': 0.1006,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task000': 7.242052741259693,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task001': 3.032204005432129,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task002': 3.314279815673828,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task003': 3.098995807647705,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task004': 3.020626127624512},\n",
       " {'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9707627118644068,\n",
       "  'Loss_Epoch/train_phase/train_stream/Task000': 0.09298881272138175,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.934135892951631,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001': 0.1054,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task001/Exp001': 3.032204005432129,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002': 0.095,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task002/Exp002': 3.314279815673828,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003': 0.0869,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task003/Exp003': 3.098995807647705,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004': 0.1006,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task004/Exp004': 3.020626127624512,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.0,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task001': 0.1054,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task002': 0.095,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task003': 0.0869,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task004': 0.1006,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task000': 7.934135892951631,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task001': 3.032204005432129,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task002': 3.314279815673828,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task003': 3.098995807647705,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task004': 3.020626127624512},\n",
       " {'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9444252077562327,\n",
       "  'Loss_Epoch/train_phase/train_stream/Task000': 0.15546714092431968,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 8.334802813738941,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001': 0.1054,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task001/Exp001': 3.032204005432129,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002': 0.095,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task002/Exp002': 3.314279815673828,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003': 0.0869,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task003/Exp003': 3.098995807647705,\n",
       "  'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004': 0.1006,\n",
       "  'Loss_Exp/eval_phase/test_stream/Task004/Exp004': 3.020626127624512,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.0,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task001': 0.1054,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task002': 0.095,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task003': 0.0869,\n",
       "  'Top1_Acc_Stream/eval_phase/test_stream/Task004': 0.1006,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task000': 8.334802813738941,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task001': 3.032204005432129,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task002': 3.314279815673828,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task003': 3.098995807647705,\n",
       "  'Loss_Stream/eval_phase/test_stream/Task004': 3.020626127624512}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4baa516f8e0b375803eea76c457b4bf97bb771c396b3a29908f0ad5250be2f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
