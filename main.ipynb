{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement https://arxiv.org/pdf/1705.08690.pdf on avalanche framework on permuted MNIST\n",
    "# https://aahaanmaini.medium.com/mimicking-human-continual-learning-in-a-neural-network-c15e1ae11d70\n",
    "#continual learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN/blob/master/pytorch_MNIST_GAN.py\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "#import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# G(z)\n",
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_size=32, n_class = 10):\n",
    "        super(generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, 1024)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.fc1(input), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.tanh(self.fc4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_size=32, n_class=10):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, 256)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.fc1(input), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "#MNIST neural network with 2 hidden layers of 400 neurons each\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 400)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.fc3 = nn.Linear(400, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = Net()\n",
    "optimizer_naive = SGD(model_naive.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion_naive = CrossEntropyLoss()\n",
    "\n",
    "naive_accuracies = []\n",
    "for experience in range(5):\n",
    "    train_dataset,test_dataset = get_datasets(experience, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "    for epoch in range(0, 3):\n",
    "        model_naive.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #wrap in variable\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer_naive.zero_grad()\n",
    "            output = model_naive(data)\n",
    "            loss = criterion_naive(output, target)\n",
    "            loss.backward()\n",
    "            optimizer_naive.step()\n",
    "    \n",
    "    model_naive.eval()\n",
    "    accuracy = test_model(model_naive, test_loader)\n",
    "    print('Experience: {} Accuracy: {:.2f}'.format(experience, accuracy))\n",
    "    naive_accuracies.append(accuracy)\n",
    "\n",
    "print(naive_accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scholar with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_func import *\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def get_new_generator_and_discriminator():\n",
    "    lr = 0.0002\n",
    "    G = generator(input_size=100, n_class=28*28)\n",
    "    D = discriminator(input_size=28*28, n_class=1)\n",
    "\n",
    "    G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "    D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "    BCE_loss = nn.BCELoss()\n",
    "    return G, D, G_optimizer, D_optimizer, BCE_loss\n",
    "\n",
    "# training parameters\n",
    "batch_size = 128\n",
    "#fixed_z_ = torch.randn((batch_size, 100))\n",
    "\n",
    "\n",
    "#train CL\n",
    "cl_accuracies = []\n",
    "for experience in range(5):\n",
    "    train_dataset, test_dataset = get_datasets(experience, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "    if experience != 0:\n",
    "        #create new data with GAN\n",
    "        new_dataset = generate_data(G, model_cl, n_images = 60000*2/10, batch_size = batch_size) # half of the images are generated half are new\n",
    "        #concatenate with old data\n",
    "        X = torch.cat((train_dataset.data, new_dataset.data), 0)\n",
    "        y = torch.cat((train_dataset.targets, new_dataset.targets), 0)\n",
    "        #create new dataset\n",
    "        concat_dataset = DS_from_tensors(X, y)\n",
    "        train_loader = torch.utils.data.DataLoader(concat_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('training GAN on experience {}'.format(experience))\n",
    "    G, D, G_optimizer, D_optimizer, BCE_loss = get_new_generator_and_discriminator()\n",
    "    G, D, D_losses, G_losses = train_GAN(G, D, train_loader, G_optimizer, D_optimizer, BCE_loss, train_epoch = 200)\n",
    "    images = generate_images(G, n_images=25).numpy()\n",
    "    plt.imshow(images.transpose((1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "    #CL train\n",
    "    model_cl = Net()\n",
    "    cl_optimizer = SGD(model_cl.parameters(), lr=0.001, momentum=0.9)\n",
    "    cl_criterion = CrossEntropyLoss()\n",
    "    for epoch in range(3):\n",
    "        model_cl.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #wrap data in Variable\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            cl_optimizer.zero_grad()\n",
    "            output = model_cl(data)\n",
    "            loss = cl_criterion(output, target)\n",
    "            loss.backward()\n",
    "            cl_optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "    model_cl.eval()\n",
    "    accuracy = test_model(model_cl, test_loader)\n",
    "    cl_accuracies.append(accuracy)\n",
    "\n",
    "print('cl accuracies: {}'.format(cl_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(naive_accuracies, label='naive')\n",
    "plt.plot(cl_accuracies, label='CL')\n",
    "\n",
    "plt.xlabel('Task')\n",
    "\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4baa516f8e0b375803eea76c457b4bf97bb771c396b3a29908f0ad5250be2f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
